{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Business Analytics Centre Democratizing Analytics, Improving Collaborations, Industry Focused Industry Engagement You can use analytics to get a better understanding of your customers, track which channels are most effective for marketing, find out how customers are engaging with your product or service and more. Contact us for more information. Curricula Development Curricula Development of business analytics modules is a time-consuming process that needs to include industry experts, academic, and researchers from various backgrounds. We can provide you with advice on what to include in your curriculum development plan and how to put together Expertise in Analytics and Visualization Visualization is a great way to communicate complex concepts and ideas in a simple way. Analytics can be presented in any form, from charts, graphs, infographics, maps, etcetera depending on the type of data available and the purpose of visualization. iSOLT Opportunities There are many new opportunities for educators to engage in research. The subjects involved are not only diverse and complicated, but also overlapping. This makes it hard to figure out what goes into an analytics curriculum and how the subject should be taught.","title":"Home"},{"location":"contact/","text":"Contact US A.Prof Nik Thompson Dr Luke Butcher Dr Michael Borck","title":"Contact US"},{"location":"contact/#contact-us","text":"A.Prof Nik Thompson Dr Luke Butcher Dr Michael Borck","title":"Contact US"},{"location":"roadmap/","text":"Roadmap Business Analytics \u2018Centre\u2019 Pull together expertise in Analytics and visualization in Business Develop brief bios and expertise of core members (maybe 3 to start with?) Associate members brought in for Case by Case projects Broad \u2018project lifecycle\u2019 i.e. back end (data collection, method, IS) through to front end (analysis, visuals, insights, and strategy) Autonomous Hub Develop a Profile online and internally for our expertise and sorts of problems we can help solve Source particular software required Talkwalker Google Analytics Social Analytics Mailchimp style Develop Case Studies of what we can do / have done Research and teaching Purposes Workspace/Hub/Showcase 408.1011 Utilize this space for various events / teaching / industry engagement Marketing to prospective students / agents / industry partners New name: \u2018Digital Analytics Hub\u2019 Intensive teaching \u2013 simulations Re-configure Space: Control the 9 Monitor Stack on wall Breakout zones \u2013 own monitors, dividers, white boards, brainstorming, desks, charging Share BZs screen onto main screen \u2013 or just HDMI may suffice? Modular furniture for up to 6 groups Hinged monitor so can lay flat for better collab / remove power dynamics Improve lighting Currently looks good in room, but terrible on recording / distributed or vice versa Ability to let people in freely during class \u2013 not everyone locked out Curricula Develop modules for teaching these skills across SOMM Courses Clear structure at COURSE level Scaffolded and enshrined Need to Map what we currently have across courses (formal, informal \u2013what level) What, where, when Intentional development Fill gaps Develop supplementary curricula available through BB for students who need to skill up / don\u2019t have requisite learning Get Students \u2018Behind the Wheel\u2019 \u2013 develop Curricula for them to put into practice (Authentic Assessments) iSOLT Case Study Mapping, developing, etc Curtin units, programs, assets etc as pilot studies for what we can do Industry engagement Use the Space and Our Expertise for commercial purposes with industry partners In exchange for in-kind support Student projects, PD, Staff Dev, Internships, etc. Address issues of bureaucrac raised through \u2018Ignite Search\u2019 failed event Executive Education / Curtin Credentials Bigger Picture Develop as a Start Up type thing like the \u2018Consumer Research Lab\u2019 (speak to Billy) With own Cost Centre Bring in RAs to work Synergies Curtin Innovative Computing (CIC) HIVE Cross collaborate on projects \u2013 utilize strategically at different phases of the project Consumer Research Lab, Design Thinking Lab Accelerate program","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#business-analytics-centre","text":"Pull together expertise in Analytics and visualization in Business Develop brief bios and expertise of core members (maybe 3 to start with?) Associate members brought in for Case by Case projects Broad \u2018project lifecycle\u2019 i.e. back end (data collection, method, IS) through to front end (analysis, visuals, insights, and strategy) Autonomous Hub Develop a Profile online and internally for our expertise and sorts of problems we can help solve Source particular software required Talkwalker Google Analytics Social Analytics Mailchimp style Develop Case Studies of what we can do / have done Research and teaching Purposes","title":"Business Analytics \u2018Centre\u2019"},{"location":"roadmap/#workspacehubshowcase-4081011","text":"Utilize this space for various events / teaching / industry engagement Marketing to prospective students / agents / industry partners New name: \u2018Digital Analytics Hub\u2019 Intensive teaching \u2013 simulations Re-configure Space: Control the 9 Monitor Stack on wall Breakout zones \u2013 own monitors, dividers, white boards, brainstorming, desks, charging Share BZs screen onto main screen \u2013 or just HDMI may suffice? Modular furniture for up to 6 groups Hinged monitor so can lay flat for better collab / remove power dynamics Improve lighting Currently looks good in room, but terrible on recording / distributed or vice versa Ability to let people in freely during class \u2013 not everyone locked out","title":"Workspace/Hub/Showcase 408.1011"},{"location":"roadmap/#curricula","text":"Develop modules for teaching these skills across SOMM Courses Clear structure at COURSE level Scaffolded and enshrined Need to Map what we currently have across courses (formal, informal \u2013what level) What, where, when Intentional development Fill gaps Develop supplementary curricula available through BB for students who need to skill up / don\u2019t have requisite learning Get Students \u2018Behind the Wheel\u2019 \u2013 develop Curricula for them to put into practice (Authentic Assessments)","title":"Curricula"},{"location":"roadmap/#isolt","text":"Case Study Mapping, developing, etc Curtin units, programs, assets etc as pilot studies for what we can do","title":"iSOLT"},{"location":"roadmap/#industry-engagement","text":"Use the Space and Our Expertise for commercial purposes with industry partners In exchange for in-kind support Student projects, PD, Staff Dev, Internships, etc. Address issues of bureaucrac raised through \u2018Ignite Search\u2019 failed event Executive Education / Curtin Credentials","title":"Industry engagement"},{"location":"roadmap/#bigger-picture","text":"Develop as a Start Up type thing like the \u2018Consumer Research Lab\u2019 (speak to Billy) With own Cost Centre Bring in RAs to work Synergies Curtin Innovative Computing (CIC) HIVE Cross collaborate on projects \u2013 utilize strategically at different phases of the project Consumer Research Lab, Design Thinking Lab Accelerate program","title":"Bigger Picture"},{"location":"team/","text":"TBAC Team _ Associate Professor Nik Thompson I am an Associate Professor and the Discipline Lead of Business Information Systems in Curtin Business School. Prior to Curtin I have held roles in industry as a Systems Consultant in the resources sector and in academia as a Chair of Information Security. My research and teaching converge in the area of Information Security. While my teaching is management and operations focussed, my research also considers the security of every day computer users when interacting with a range of technologies and media. See his Curtin Profile for more detail. Dr Luke Butcher I have a passion for quantitative and qualitative research, and focus my attention to innovation acceptance, luxury branding, user experiences, social media, and the evolution of the video games industry and their enigmatic gamers. My research interests extend to the cultural, social, and personal drivers of consumer behaviour and insights. See his Curtin Profile for more detail. Dr Michael Borck A experienced IT/Computer Science professional and with a talent for mastering complex situations and developing effective solutions. Research interest include Predictive Analytics, Spatial Analysis, Computer Vision, Machine Learning, Learning and Teaching, Computer Programming and Cybersecurity. See his Curtin Profile for more detail.","title":"Overview"},{"location":"team/#tbac-team","text":"_","title":"TBAC Team"},{"location":"team/#associate-professor-nik-thompson","text":"I am an Associate Professor and the Discipline Lead of Business Information Systems in Curtin Business School. Prior to Curtin I have held roles in industry as a Systems Consultant in the resources sector and in academia as a Chair of Information Security. My research and teaching converge in the area of Information Security. While my teaching is management and operations focussed, my research also considers the security of every day computer users when interacting with a range of technologies and media. See his Curtin Profile for more detail.","title":"Associate Professor Nik Thompson"},{"location":"team/#dr-luke-butcher","text":"I have a passion for quantitative and qualitative research, and focus my attention to innovation acceptance, luxury branding, user experiences, social media, and the evolution of the video games industry and their enigmatic gamers. My research interests extend to the cultural, social, and personal drivers of consumer behaviour and insights. See his Curtin Profile for more detail.","title":"Dr Luke Butcher"},{"location":"team/#dr-michael-borck","text":"A experienced IT/Computer Science professional and with a talent for mastering complex situations and developing effective solutions. Research interest include Predictive Analytics, Spatial Analysis, Computer Vision, Machine Learning, Learning and Teaching, Computer Programming and Cybersecurity. See his Curtin Profile for more detail.","title":"Dr Michael Borck"},{"location":"showcase/","text":"Showcase Examples of various types of analytics and how they are used in the world of marketing, social media, and other areas. Learning Analytics The world is changing. Communication is moving to digital boards, and communities are coalescing around these boards. They have also become a common interactive tool in the learning and teaching environment. Discussion Boards are an interesting place to examine how people interact with each other online using different strategies that may be best suited to their personality or position on a topic. With this notebook, you can easily visualize and understand the interaction and mood of the discussion. Brand Analytics Monitoring audience behaviour on social media has so many benefits. For instance, it can help you identify what your target audience likes, what their preferences are, and what they might be interested in hearing more about. It can also show you who your competitors are and how well they are doing compared to you. Customer Churn Tracking churn rate is a useful tool that businesses can use to improve their customer retention. A high turnover rate (churn rate) can be detrimental for any business, as it translates into a significant loss of revenue. In order to evaluate the reasons for this turnover, it is necessary to track and monitor the churn rate closely. This example demonstrates the lifecycle of the Cross-industry Standard Process for Data Mining (CRISP-DM). Visual Decisions Making People who make decisions need to know what information is relevant and what is not. Plotting your data can help you spot patterns and trends that you might not otherwise notice, and help you make better decisions. Automatic PCA Analysis PCA is a technique for transforming a set of variables into a set of uncorrelated variables. PCA can be used for exploratory data analysis, dimensionality reduction, feature extraction, and many other tasks. PCA is a linear transformation so it can be implemented efficiently and it\u2019s the most commonly used algorithm for dimensionality reduction.","title":"Overview"},{"location":"showcase/#showcase","text":"Examples of various types of analytics and how they are used in the world of marketing, social media, and other areas.","title":"Showcase"},{"location":"showcase/#learning-analytics","text":"The world is changing. Communication is moving to digital boards, and communities are coalescing around these boards. They have also become a common interactive tool in the learning and teaching environment. Discussion Boards are an interesting place to examine how people interact with each other online using different strategies that may be best suited to their personality or position on a topic. With this notebook, you can easily visualize and understand the interaction and mood of the discussion.","title":"Learning Analytics"},{"location":"showcase/#brand-analytics","text":"Monitoring audience behaviour on social media has so many benefits. For instance, it can help you identify what your target audience likes, what their preferences are, and what they might be interested in hearing more about. It can also show you who your competitors are and how well they are doing compared to you.","title":"Brand Analytics"},{"location":"showcase/#customer-churn","text":"Tracking churn rate is a useful tool that businesses can use to improve their customer retention. A high turnover rate (churn rate) can be detrimental for any business, as it translates into a significant loss of revenue. In order to evaluate the reasons for this turnover, it is necessary to track and monitor the churn rate closely. This example demonstrates the lifecycle of the Cross-industry Standard Process for Data Mining (CRISP-DM).","title":"Customer Churn"},{"location":"showcase/#visual-decisions-making","text":"People who make decisions need to know what information is relevant and what is not. Plotting your data can help you spot patterns and trends that you might not otherwise notice, and help you make better decisions.","title":"Visual Decisions Making"},{"location":"showcase/#automatic-pca-analysis","text":"PCA is a technique for transforming a set of variables into a set of uncorrelated variables. PCA can be used for exploratory data analysis, dimensionality reduction, feature extraction, and many other tasks. PCA is a linear transformation so it can be implemented efficiently and it\u2019s the most commonly used algorithm for dimensionality reduction.","title":"Automatic PCA Analysis"},{"location":"showcase/brand-analytics/brand-analytics/","text":"Brand Analytics (for beginners) Marketing Anlaytics Monitoring audience behaviour on social media has so many benefits. For instance, it can help you identify what your target audience likes, what their preferences are, and what they might be interested in hearing more about. It can also show you who your competitors are and how well they are doing compared to you. This app uses the standard twitter API. The standard API only allows you to retrieve tweets up to 7 days ago and is limited to scraping 18,000 tweets per a 15 minute window. A Twitter Sentiment analysis project which will scrap twitter for the topic selected by the user. The extracted tweets will then be used to determine the Sentiments of those tweets. The different Visualizations will help us get a feel of the overall mood of the people on Twitter. Try Brandwatch Step 1: Type in search term Step 2: See the trend","title":"Brand Analytics"},{"location":"showcase/brand-analytics/brand-analytics/#brand-analytics-for-beginners","text":"Marketing Anlaytics Monitoring audience behaviour on social media has so many benefits. For instance, it can help you identify what your target audience likes, what their preferences are, and what they might be interested in hearing more about. It can also show you who your competitors are and how well they are doing compared to you. This app uses the standard twitter API. The standard API only allows you to retrieve tweets up to 7 days ago and is limited to scraping 18,000 tweets per a 15 minute window. A Twitter Sentiment analysis project which will scrap twitter for the topic selected by the user. The extracted tweets will then be used to determine the Sentiments of those tweets. The different Visualizations will help us get a feel of the overall mood of the people on Twitter.","title":"Brand Analytics (for beginners)"},{"location":"showcase/brand-analytics/brand-analytics/#try-brandwatch","text":"","title":"Try Brandwatch"},{"location":"showcase/brand-analytics/brand-analytics/#step-1-type-in-search-term","text":"","title":"Step 1: Type in search term"},{"location":"showcase/brand-analytics/brand-analytics/#step-2-see-the-trend","text":"","title":"Step 2: See the trend"},{"location":"showcase/customer-churn/00_crisp-dm/00_crisp-dm/","text":"Customer Churn CRISP-DM Lifecycle Tracking churn rate is a useful tool that businesses can use to improve their customer retention. A high turnover rate (churn rate) can be detrimental for any business, as it translates into a significant loss of revenue. In order to evaluate the reasons for this turnover, it is necessary to track and monitor the churn rate closely. This example demonstrates the complete lifecycle of the Cross-industry Standard Process for Data Mining (CRISP-DM). CRISP-DM The most widely-used analytics model. Business understanding Data understanding Data preparation Modelling Evaluation Deployment Process diagram showing the relationship between the different phases of CRISP-DM Source: Wikipedia","title":"CRISP-DM Overview"},{"location":"showcase/customer-churn/00_crisp-dm/00_crisp-dm/#customer-churn","text":"CRISP-DM Lifecycle Tracking churn rate is a useful tool that businesses can use to improve their customer retention. A high turnover rate (churn rate) can be detrimental for any business, as it translates into a significant loss of revenue. In order to evaluate the reasons for this turnover, it is necessary to track and monitor the churn rate closely. This example demonstrates the complete lifecycle of the Cross-industry Standard Process for Data Mining (CRISP-DM).","title":"Customer Churn"},{"location":"showcase/customer-churn/00_crisp-dm/00_crisp-dm/#crisp-dm","text":"The most widely-used analytics model. Business understanding Data understanding Data preparation Modelling Evaluation Deployment Process diagram showing the relationship between the different phases of CRISP-DM Source: Wikipedia","title":"CRISP-DM"},{"location":"showcase/customer-churn/01_business-understanding/01_business-understanding/","text":"Business Understanding Stage One Tracking churn rate is a useful tool that businesses can use to improve their customer retention. A high turnover rate (churn rate) can be detrimental for any business, as it translates into a significant loss of revenue. In order to evaluate the reasons for this turnover, it is necessary to track and monitor the churn rate closely. Preventing churn is the responsibility of every team within the company. It\u2019s up to the marketing team to properly educate new buyers, the sales team to not overset expectations, the customer success team to provide high quality support, and the product team to build a service that continues to delight customers. Companies can improve their churn rates by sharing churn data throughout the company. The more insights each team has into how it can reduce churn, the more customers the whole company keeps. The outputs of this step are: Set objectives \u2013 This means describing your primary objective from a business perspective. There may also be other related questions that you would like to address. For example, your primary goal might be to keep current customers by predicting when they are prone to move to a competitor. Related business questions might be \u201cDoes the channel used affect whether customers stay or go?\u201d or \u201cWill lower ATM fees significantly reduce the number of high-value customers who leave?\u201d Produce project plan \u2013 Here you\u2019ll describe the plan for achieving the data mining and business goals. The plan should specify the steps to be performed during the rest of the project, including the initial selection of tools and techniques. Business success criteria \u2013 Here you\u2019ll lay out the criteria that you\u2019ll use to determine whether the project has been successful from the business point of view. These should ideally be specific and measurable, for example reduction of customer churn to a certain level, however sometimes it might be necessary to have more subjective criteria such as \u201cgive useful insights into the relationships.\u201d If this is the case then it needs to be clear who it is that makes the subjective judgment.","title":"Business Understanding"},{"location":"showcase/customer-churn/01_business-understanding/01_business-understanding/#business-understanding","text":"Stage One Tracking churn rate is a useful tool that businesses can use to improve their customer retention. A high turnover rate (churn rate) can be detrimental for any business, as it translates into a significant loss of revenue. In order to evaluate the reasons for this turnover, it is necessary to track and monitor the churn rate closely. Preventing churn is the responsibility of every team within the company. It\u2019s up to the marketing team to properly educate new buyers, the sales team to not overset expectations, the customer success team to provide high quality support, and the product team to build a service that continues to delight customers. Companies can improve their churn rates by sharing churn data throughout the company. The more insights each team has into how it can reduce churn, the more customers the whole company keeps. The outputs of this step are: Set objectives \u2013 This means describing your primary objective from a business perspective. There may also be other related questions that you would like to address. For example, your primary goal might be to keep current customers by predicting when they are prone to move to a competitor. Related business questions might be \u201cDoes the channel used affect whether customers stay or go?\u201d or \u201cWill lower ATM fees significantly reduce the number of high-value customers who leave?\u201d Produce project plan \u2013 Here you\u2019ll describe the plan for achieving the data mining and business goals. The plan should specify the steps to be performed during the rest of the project, including the initial selection of tools and techniques. Business success criteria \u2013 Here you\u2019ll lay out the criteria that you\u2019ll use to determine whether the project has been successful from the business point of view. These should ideally be specific and measurable, for example reduction of customer churn to a certain level, however sometimes it might be necessary to have more subjective criteria such as \u201cgive useful insights into the relationships.\u201d If this is the case then it needs to be clear who it is that makes the subjective judgment.","title":"Business Understanding"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/","text":"Data Understanding Stage 2 Telco Customer Churn The data set includes information about: Customers who left within the last month \u2013 the column is called Churn Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges Demographic info about customers \u2013 gender, age range, and if they have partners and dependents Each row represents a customer, each column contains customer\u2019s attributes described on the column Metadata. See Telco Customer Churn for more information on the data and other notebooks. Churn Count Based on gender Based on service Based on contract type Based on payment method From the two charts above, I can clearly see that there is some discrimination in the data. The monthly charges chart (on the left) shows that most of the loyal customers that stayed with the company had a monthly charge between $20 and $30. Most of the customers that churned had a monthly charge of $70 to $100. Maybe the company should lower the monthly charges to retain customers. The tenure chart (on the right) shows some discrimination as well. From the chart, I can see that most of the customers that churned had between 1 and 9 months with the company, while most of the retained customers had a tenure between 24 and 72 months which is 2 to 6 years. So, it may be in the companies best interest to try everything they can to keep their customers for at least 2 years. Verify data quality Examine the quality of the data, addressing questions such as: Is the data complete (does it cover all the cases required)? Do the data type make sense? Is it correct, or does it contain errors and, if there are errors, how common are they? Are there missing values in the data? If so, how are they represented, where do they occur, and how common are they? How many unique values are in the data set? Are there duplicate values? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Data Type Missing Values Unique Values Maximum Value customerID object 0 7043 9995-HOTOH gender object 0 2 Male SeniorCitizen int64 0 2 1 Partner object 0 2 Yes Dependents object 0 2 Yes tenure int64 0 73 72 PhoneService object 0 2 Yes MultipleLines object 0 3 Yes InternetService object 0 3 No OnlineSecurity object 0 3 Yes OnlineBackup object 0 3 Yes DeviceProtection object 0 3 Yes TechSupport object 0 3 Yes StreamingTV object 0 3 Yes StreamingMovies object 0 3 Yes Contract object 0 3 Two year PaperlessBilling object 0 2 Yes PaymentMethod object 0 4 Mailed check MonthlyCharges float64 0 1585 118.75 TotalCharges object 0 6531 999.9 Churn object 0 2 Yes","title":"Data Understanding"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/#data-understanding","text":"Stage 2","title":"Data Understanding"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/#telco-customer-churn","text":"The data set includes information about: Customers who left within the last month \u2013 the column is called Churn Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges Demographic info about customers \u2013 gender, age range, and if they have partners and dependents Each row represents a customer, each column contains customer\u2019s attributes described on the column Metadata. See Telco Customer Churn for more information on the data and other notebooks.","title":"Telco Customer Churn"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/#churn-count","text":"","title":"Churn Count"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/#based-on-gender","text":"","title":"Based on gender"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/#based-on-service","text":"","title":"Based on service"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/#based-on-contract-type","text":"","title":"Based on contract type"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/#based-on-payment-method","text":"From the two charts above, I can clearly see that there is some discrimination in the data. The monthly charges chart (on the left) shows that most of the loyal customers that stayed with the company had a monthly charge between $20 and $30. Most of the customers that churned had a monthly charge of $70 to $100. Maybe the company should lower the monthly charges to retain customers. The tenure chart (on the right) shows some discrimination as well. From the chart, I can see that most of the customers that churned had between 1 and 9 months with the company, while most of the retained customers had a tenure between 24 and 72 months which is 2 to 6 years. So, it may be in the companies best interest to try everything they can to keep their customers for at least 2 years.","title":"Based on payment method"},{"location":"showcase/customer-churn/02_data-understanding/02_data-understanding/#verify-data-quality","text":"Examine the quality of the data, addressing questions such as: Is the data complete (does it cover all the cases required)? Do the data type make sense? Is it correct, or does it contain errors and, if there are errors, how common are they? Are there missing values in the data? If so, how are they represented, where do they occur, and how common are they? How many unique values are in the data set? Are there duplicate values? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Data Type Missing Values Unique Values Maximum Value customerID object 0 7043 9995-HOTOH gender object 0 2 Male SeniorCitizen int64 0 2 1 Partner object 0 2 Yes Dependents object 0 2 Yes tenure int64 0 73 72 PhoneService object 0 2 Yes MultipleLines object 0 3 Yes InternetService object 0 3 No OnlineSecurity object 0 3 Yes OnlineBackup object 0 3 Yes DeviceProtection object 0 3 Yes TechSupport object 0 3 Yes StreamingTV object 0 3 Yes StreamingMovies object 0 3 Yes Contract object 0 3 Two year PaperlessBilling object 0 2 Yes PaymentMethod object 0 4 Mailed check MonthlyCharges float64 0 1585 118.75 TotalCharges object 0 6531 999.9 Churn object 0 2 Yes","title":"Verify data quality"},{"location":"showcase/customer-churn/03_data-preparation/03_data-preparation/","text":"Data Preparation Stage 3 This phase, which is often referred to as \u201cdata munging\u201d, prepares the final data set(s) for modeling. It has five tasks: Select data: Determine which data sets will be used and document reasons for inclusion/exclusion. Clean data: Often this is the lengthiest task. Without it, you\u2019ll likely fall victim to garbage-in, garbage-out. A common practice during this task is to correct, impute, or remove erroneous values. Construct data: Derive new attributes that will be helpful. For example, derive someone\u2019s body mass index from height and weight fields. Integrate data: Create new data sets by combining data from multiple sources. Format data: Re-format data as necessary. For example, you might convert string values that store numbers to numeric values so that you can perform mathematical operations. Machine Learning algorithms can typically only have numerical values as their independent variables. Label encoding is quite pivotal as they encode categorical labels with appropriate numerical values. Here we are label encoding all categorical variables that have unique values. df.dtypes gender object SeniorCitizen int64 Partner object Dependents object tenure int64 PhoneService object MultipleLines object InternetService object OnlineSecurity object OnlineBackup object DeviceProtection object TechSupport object StreamingTV object StreamingMovies object Contract object PaperlessBilling object PaymentMethod object MonthlyCharges float64 TotalCharges object Churn object dtype: object Our customer churn data set is clean, no missing values, no nulls, no duplicates but still needs to be transformed for use by machine learning algorithms. The following need to be done before one-hot-encoding: Delete the CustomerID column. The seniorCitizen column only contains two values, 0 or 1. It is currently listed as a numeric type. The TotalCharges column is a generic type and need to be converted into a numeric Preserve the 'Churn' column as this is the column we are trying to predict. del df['customerID'] df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce') df['TotalCharges'] = df['TotalCharges'].astype(\"float\") df['SeniorCitizen']=pd.Categorical(df['SeniorCitizen']) df.dtypes gender object SeniorCitizen category Partner object Dependents object tenure int64 PhoneService object MultipleLines object InternetService object OnlineSecurity object OnlineBackup object DeviceProtection object TechSupport object StreamingTV object StreamingMovies object Contract object PaperlessBilling object PaymentMethod object MonthlyCharges float64 TotalCharges float64 Churn object dtype: object Once all columns are the correct data type then proceed to one-hot-encode. churn = df['Churn'] del df['Churn'] df= pd.get_dummies(df) df['Churn'] = churn df.dtypes tenure int64 MonthlyCharges float64 TotalCharges float64 gender_Female uint8 gender_Male uint8 SeniorCitizen_0 uint8 SeniorCitizen_1 uint8 Partner_No uint8 Partner_Yes uint8 Dependents_No uint8 Dependents_Yes uint8 PhoneService_No uint8 PhoneService_Yes uint8 MultipleLines_No uint8 MultipleLines_No phone service uint8 MultipleLines_Yes uint8 InternetService_DSL uint8 InternetService_Fiber optic uint8 InternetService_No uint8 OnlineSecurity_No uint8 OnlineSecurity_No internet service uint8 OnlineSecurity_Yes uint8 OnlineBackup_No uint8 OnlineBackup_No internet service uint8 OnlineBackup_Yes uint8 DeviceProtection_No uint8 DeviceProtection_No internet service uint8 DeviceProtection_Yes uint8 TechSupport_No uint8 TechSupport_No internet service uint8 TechSupport_Yes uint8 StreamingTV_No uint8 StreamingTV_No internet service uint8 StreamingTV_Yes uint8 StreamingMovies_No uint8 StreamingMovies_No internet service uint8 StreamingMovies_Yes uint8 Contract_Month-to-month uint8 Contract_One year uint8 Contract_Two year uint8 PaperlessBilling_No uint8 PaperlessBilling_Yes uint8 PaymentMethod_Bank transfer (automatic) uint8 PaymentMethod_Credit card (automatic) uint8 PaymentMethod_Electronic check uint8 PaymentMethod_Mailed check uint8 Churn object dtype: object","title":"Data Preparation"},{"location":"showcase/customer-churn/03_data-preparation/03_data-preparation/#data-preparation","text":"Stage 3 This phase, which is often referred to as \u201cdata munging\u201d, prepares the final data set(s) for modeling. It has five tasks: Select data: Determine which data sets will be used and document reasons for inclusion/exclusion. Clean data: Often this is the lengthiest task. Without it, you\u2019ll likely fall victim to garbage-in, garbage-out. A common practice during this task is to correct, impute, or remove erroneous values. Construct data: Derive new attributes that will be helpful. For example, derive someone\u2019s body mass index from height and weight fields. Integrate data: Create new data sets by combining data from multiple sources. Format data: Re-format data as necessary. For example, you might convert string values that store numbers to numeric values so that you can perform mathematical operations. Machine Learning algorithms can typically only have numerical values as their independent variables. Label encoding is quite pivotal as they encode categorical labels with appropriate numerical values. Here we are label encoding all categorical variables that have unique values. df.dtypes gender object SeniorCitizen int64 Partner object Dependents object tenure int64 PhoneService object MultipleLines object InternetService object OnlineSecurity object OnlineBackup object DeviceProtection object TechSupport object StreamingTV object StreamingMovies object Contract object PaperlessBilling object PaymentMethod object MonthlyCharges float64 TotalCharges object Churn object dtype: object Our customer churn data set is clean, no missing values, no nulls, no duplicates but still needs to be transformed for use by machine learning algorithms. The following need to be done before one-hot-encoding: Delete the CustomerID column. The seniorCitizen column only contains two values, 0 or 1. It is currently listed as a numeric type. The TotalCharges column is a generic type and need to be converted into a numeric Preserve the 'Churn' column as this is the column we are trying to predict. del df['customerID'] df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce') df['TotalCharges'] = df['TotalCharges'].astype(\"float\") df['SeniorCitizen']=pd.Categorical(df['SeniorCitizen']) df.dtypes gender object SeniorCitizen category Partner object Dependents object tenure int64 PhoneService object MultipleLines object InternetService object OnlineSecurity object OnlineBackup object DeviceProtection object TechSupport object StreamingTV object StreamingMovies object Contract object PaperlessBilling object PaymentMethod object MonthlyCharges float64 TotalCharges float64 Churn object dtype: object Once all columns are the correct data type then proceed to one-hot-encode. churn = df['Churn'] del df['Churn'] df= pd.get_dummies(df) df['Churn'] = churn df.dtypes tenure int64 MonthlyCharges float64 TotalCharges float64 gender_Female uint8 gender_Male uint8 SeniorCitizen_0 uint8 SeniorCitizen_1 uint8 Partner_No uint8 Partner_Yes uint8 Dependents_No uint8 Dependents_Yes uint8 PhoneService_No uint8 PhoneService_Yes uint8 MultipleLines_No uint8 MultipleLines_No phone service uint8 MultipleLines_Yes uint8 InternetService_DSL uint8 InternetService_Fiber optic uint8 InternetService_No uint8 OnlineSecurity_No uint8 OnlineSecurity_No internet service uint8 OnlineSecurity_Yes uint8 OnlineBackup_No uint8 OnlineBackup_No internet service uint8 OnlineBackup_Yes uint8 DeviceProtection_No uint8 DeviceProtection_No internet service uint8 DeviceProtection_Yes uint8 TechSupport_No uint8 TechSupport_No internet service uint8 TechSupport_Yes uint8 StreamingTV_No uint8 StreamingTV_No internet service uint8 StreamingTV_Yes uint8 StreamingMovies_No uint8 StreamingMovies_No internet service uint8 StreamingMovies_Yes uint8 Contract_Month-to-month uint8 Contract_One year uint8 Contract_Two year uint8 PaperlessBilling_No uint8 PaperlessBilling_Yes uint8 PaymentMethod_Bank transfer (automatic) uint8 PaymentMethod_Credit card (automatic) uint8 PaymentMethod_Electronic check uint8 PaymentMethod_Mailed check uint8 Churn object dtype: object","title":"Data Preparation"},{"location":"showcase/customer-churn/04_modeling/04_modeling/","text":"Modeling Stage 4 Churn prediction modeling techniques attempt to understand the precise customer behaviors and attributes which signal the risk and timing of customer churn. The accuracy of the technique used is obviously critical to the success of any proactive retention efforts. The most common churn prediction models are based on statistical and data-mining methods, such as logistic regression and other binary modeling techniques. Out data has been prepared, we will withhold a random sample of 10% of the data. We will use this to evaluate the final model. We use the PyCaret library. We must set the dataset via the setup() function. This requires that we provide the Pandas DataFrame and specify the name of the column that contains the target variable. The setup() function also allows you to configure simple data pipeline, such as scaling, power transforms, missing data handling, and PCA transforms. Next, we can compare standard machine learning models by calling the compare_models() function. By default, it will evaluate models using 10-fold cross-validation, sort results by classification accuracy, and return the single best model. These are good defaults, and we don\u2019t need to change a thing. Compare Models #T_1314f_ th { text-align: left; }#T_1314f_row0_col0,#T_1314f_row0_col3,#T_1314f_row0_col4,#T_1314f_row0_col5,#T_1314f_row0_col6,#T_1314f_row0_col7,#T_1314f_row1_col0,#T_1314f_row1_col1,#T_1314f_row1_col2,#T_1314f_row1_col3,#T_1314f_row1_col4,#T_1314f_row1_col5,#T_1314f_row2_col0,#T_1314f_row2_col1,#T_1314f_row2_col2,#T_1314f_row2_col3,#T_1314f_row2_col5,#T_1314f_row2_col6,#T_1314f_row2_col7,#T_1314f_row3_col0,#T_1314f_row3_col1,#T_1314f_row3_col2,#T_1314f_row3_col3,#T_1314f_row3_col4,#T_1314f_row3_col5,#T_1314f_row3_col6,#T_1314f_row3_col7,#T_1314f_row4_col0,#T_1314f_row4_col1,#T_1314f_row4_col2,#T_1314f_row4_col3,#T_1314f_row4_col4,#T_1314f_row4_col5,#T_1314f_row4_col6,#T_1314f_row4_col7,#T_1314f_row5_col0,#T_1314f_row5_col1,#T_1314f_row5_col2,#T_1314f_row5_col3,#T_1314f_row5_col4,#T_1314f_row5_col5,#T_1314f_row5_col6,#T_1314f_row5_col7,#T_1314f_row6_col0,#T_1314f_row6_col1,#T_1314f_row6_col2,#T_1314f_row6_col3,#T_1314f_row6_col4,#T_1314f_row6_col5,#T_1314f_row6_col6,#T_1314f_row6_col7,#T_1314f_row7_col0,#T_1314f_row7_col1,#T_1314f_row7_col2,#T_1314f_row7_col3,#T_1314f_row7_col4,#T_1314f_row7_col5,#T_1314f_row7_col6,#T_1314f_row7_col7,#T_1314f_row8_col0,#T_1314f_row8_col1,#T_1314f_row8_col2,#T_1314f_row8_col3,#T_1314f_row8_col4,#T_1314f_row8_col5,#T_1314f_row8_col6,#T_1314f_row8_col7,#T_1314f_row9_col0,#T_1314f_row9_col1,#T_1314f_row9_col2,#T_1314f_row9_col3,#T_1314f_row9_col4,#T_1314f_row9_col5,#T_1314f_row9_col6,#T_1314f_row9_col7,#T_1314f_row10_col0,#T_1314f_row10_col1,#T_1314f_row10_col2,#T_1314f_row10_col3,#T_1314f_row10_col4,#T_1314f_row10_col5,#T_1314f_row10_col6,#T_1314f_row10_col7,#T_1314f_row11_col0,#T_1314f_row11_col1,#T_1314f_row11_col2,#T_1314f_row11_col3,#T_1314f_row11_col4,#T_1314f_row11_col5,#T_1314f_row11_col6,#T_1314f_row11_col7,#T_1314f_row12_col0,#T_1314f_row12_col1,#T_1314f_row12_col2,#T_1314f_row12_col4,#T_1314f_row12_col6,#T_1314f_row12_col7,#T_1314f_row13_col0,#T_1314f_row13_col1,#T_1314f_row13_col2,#T_1314f_row13_col3,#T_1314f_row13_col4,#T_1314f_row13_col5,#T_1314f_row13_col6,#T_1314f_row13_col7{ text-align: left; text-align: left; }#T_1314f_row0_col1,#T_1314f_row0_col2,#T_1314f_row1_col6,#T_1314f_row1_col7,#T_1314f_row2_col4,#T_1314f_row12_col3,#T_1314f_row12_col5{ text-align: left; text-align: left; background-color: yellow; }#T_1314f_row0_col8,#T_1314f_row1_col8,#T_1314f_row2_col8,#T_1314f_row3_col8,#T_1314f_row4_col8,#T_1314f_row5_col8,#T_1314f_row6_col8,#T_1314f_row7_col8,#T_1314f_row8_col8,#T_1314f_row9_col8,#T_1314f_row11_col8,#T_1314f_row12_col8,#T_1314f_row13_col8{ text-align: left; text-align: left; background-color: lightgrey; }#T_1314f_row10_col8{ text-align: left; text-align: left; background-color: yellow; background-color: lightgrey; } Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lr Logistic Regression 0.7993 0.8409 0.5549 0.6650 0.6040 0.4713 0.4753 0.9210 lda Linear Discriminant Analysis 0.7988 0.8359 0.5661 0.6592 0.6082 0.4741 0.4771 0.0360 ridge Ridge Classifier 0.7955 0.0000 0.5163 0.6692 0.5821 0.4499 0.4570 0.0200 gbc Gradient Boosting Classifier 0.7905 0.8356 0.5198 0.6526 0.5780 0.4412 0.4466 2.2610 lightgbm Light Gradient Boosting Machine 0.7841 0.8213 0.5146 0.6363 0.5686 0.4268 0.4314 17.6720 ada Ada Boost Classifier 0.7815 0.8131 0.5180 0.6276 0.5670 0.4228 0.4266 0.6080 rf Random Forest Classifier 0.7796 0.8123 0.4623 0.6403 0.5364 0.3970 0.4063 0.6200 svm SVM - Linear Kernel 0.7782 0.0000 0.5679 0.6076 0.5812 0.4322 0.4361 0.0340 xgboost Extreme Gradient Boosting 0.7770 0.8045 0.4914 0.6231 0.5486 0.4035 0.4090 8.3540 et Extra Trees Classifier 0.7729 0.7858 0.4477 0.6250 0.5210 0.3778 0.3871 0.3010 nb Naive Bayes 0.7661 0.7751 0.6235 0.5720 0.5961 0.4320 0.4332 0.0110 knn K Neighbors Classifier 0.7625 0.7733 0.5198 0.5789 0.5472 0.3870 0.3884 0.1030 qda Quadratic Discriminant Analysis 0.7492 0.8147 0.7213 0.5354 0.6142 0.4346 0.4455 0.0180 dt Decision Tree Classifier 0.7345 0.6807 0.5326 0.5200 0.5259 0.3417 0.3419 0.1380 Print 'Best' model print(best) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2', random_state=42, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) Create Model #T_2c6c0_row10_col0,#T_2c6c0_row10_col1,#T_2c6c0_row10_col2,#T_2c6c0_row10_col3,#T_2c6c0_row10_col4,#T_2c6c0_row10_col5,#T_2c6c0_row10_col6{ background: yellow; } Accuracy AUC Recall Prec. F1 Kappa MCC 0 0.7607 0.8181 0.4359 0.5930 0.5025 0.3497 0.3569 1 0.8057 0.8357 0.5470 0.6882 0.6095 0.4824 0.4881 2 0.8128 0.8565 0.6239 0.6759 0.6489 0.5215 0.5223 3 0.8175 0.8723 0.5556 0.7222 0.6280 0.5099 0.5176 4 0.8009 0.8393 0.5897 0.6571 0.6216 0.4871 0.4884 5 0.7981 0.8504 0.6121 0.6396 0.6256 0.4874 0.4877 6 0.8029 0.8241 0.5690 0.6667 0.6140 0.4827 0.4854 7 0.7957 0.8301 0.5345 0.6596 0.5905 0.4564 0.4609 8 0.8266 0.8501 0.5690 0.7416 0.6439 0.5319 0.5401 9 0.7720 0.8328 0.5128 0.6061 0.5556 0.4036 0.4062 Mean 0.7993 0.8409 0.5549 0.6650 0.6040 0.4713 0.4753 SD 0.0189 0.0156 0.0510 0.0438 0.0423 0.0529 0.0527 lr_final = finalize_model(lr_model) save_model(lr_final,'lr_20210701') Finished loading model, total used 100 iterations Transformation Pipeline and Model Succesfully Saved","title":"Modeling"},{"location":"showcase/customer-churn/04_modeling/04_modeling/#modeling","text":"Stage 4 Churn prediction modeling techniques attempt to understand the precise customer behaviors and attributes which signal the risk and timing of customer churn. The accuracy of the technique used is obviously critical to the success of any proactive retention efforts. The most common churn prediction models are based on statistical and data-mining methods, such as logistic regression and other binary modeling techniques. Out data has been prepared, we will withhold a random sample of 10% of the data. We will use this to evaluate the final model. We use the PyCaret library. We must set the dataset via the setup() function. This requires that we provide the Pandas DataFrame and specify the name of the column that contains the target variable. The setup() function also allows you to configure simple data pipeline, such as scaling, power transforms, missing data handling, and PCA transforms. Next, we can compare standard machine learning models by calling the compare_models() function. By default, it will evaluate models using 10-fold cross-validation, sort results by classification accuracy, and return the single best model. These are good defaults, and we don\u2019t need to change a thing.","title":"Modeling"},{"location":"showcase/customer-churn/04_modeling/04_modeling/#compare-models","text":"#T_1314f_ th { text-align: left; }#T_1314f_row0_col0,#T_1314f_row0_col3,#T_1314f_row0_col4,#T_1314f_row0_col5,#T_1314f_row0_col6,#T_1314f_row0_col7,#T_1314f_row1_col0,#T_1314f_row1_col1,#T_1314f_row1_col2,#T_1314f_row1_col3,#T_1314f_row1_col4,#T_1314f_row1_col5,#T_1314f_row2_col0,#T_1314f_row2_col1,#T_1314f_row2_col2,#T_1314f_row2_col3,#T_1314f_row2_col5,#T_1314f_row2_col6,#T_1314f_row2_col7,#T_1314f_row3_col0,#T_1314f_row3_col1,#T_1314f_row3_col2,#T_1314f_row3_col3,#T_1314f_row3_col4,#T_1314f_row3_col5,#T_1314f_row3_col6,#T_1314f_row3_col7,#T_1314f_row4_col0,#T_1314f_row4_col1,#T_1314f_row4_col2,#T_1314f_row4_col3,#T_1314f_row4_col4,#T_1314f_row4_col5,#T_1314f_row4_col6,#T_1314f_row4_col7,#T_1314f_row5_col0,#T_1314f_row5_col1,#T_1314f_row5_col2,#T_1314f_row5_col3,#T_1314f_row5_col4,#T_1314f_row5_col5,#T_1314f_row5_col6,#T_1314f_row5_col7,#T_1314f_row6_col0,#T_1314f_row6_col1,#T_1314f_row6_col2,#T_1314f_row6_col3,#T_1314f_row6_col4,#T_1314f_row6_col5,#T_1314f_row6_col6,#T_1314f_row6_col7,#T_1314f_row7_col0,#T_1314f_row7_col1,#T_1314f_row7_col2,#T_1314f_row7_col3,#T_1314f_row7_col4,#T_1314f_row7_col5,#T_1314f_row7_col6,#T_1314f_row7_col7,#T_1314f_row8_col0,#T_1314f_row8_col1,#T_1314f_row8_col2,#T_1314f_row8_col3,#T_1314f_row8_col4,#T_1314f_row8_col5,#T_1314f_row8_col6,#T_1314f_row8_col7,#T_1314f_row9_col0,#T_1314f_row9_col1,#T_1314f_row9_col2,#T_1314f_row9_col3,#T_1314f_row9_col4,#T_1314f_row9_col5,#T_1314f_row9_col6,#T_1314f_row9_col7,#T_1314f_row10_col0,#T_1314f_row10_col1,#T_1314f_row10_col2,#T_1314f_row10_col3,#T_1314f_row10_col4,#T_1314f_row10_col5,#T_1314f_row10_col6,#T_1314f_row10_col7,#T_1314f_row11_col0,#T_1314f_row11_col1,#T_1314f_row11_col2,#T_1314f_row11_col3,#T_1314f_row11_col4,#T_1314f_row11_col5,#T_1314f_row11_col6,#T_1314f_row11_col7,#T_1314f_row12_col0,#T_1314f_row12_col1,#T_1314f_row12_col2,#T_1314f_row12_col4,#T_1314f_row12_col6,#T_1314f_row12_col7,#T_1314f_row13_col0,#T_1314f_row13_col1,#T_1314f_row13_col2,#T_1314f_row13_col3,#T_1314f_row13_col4,#T_1314f_row13_col5,#T_1314f_row13_col6,#T_1314f_row13_col7{ text-align: left; text-align: left; }#T_1314f_row0_col1,#T_1314f_row0_col2,#T_1314f_row1_col6,#T_1314f_row1_col7,#T_1314f_row2_col4,#T_1314f_row12_col3,#T_1314f_row12_col5{ text-align: left; text-align: left; background-color: yellow; }#T_1314f_row0_col8,#T_1314f_row1_col8,#T_1314f_row2_col8,#T_1314f_row3_col8,#T_1314f_row4_col8,#T_1314f_row5_col8,#T_1314f_row6_col8,#T_1314f_row7_col8,#T_1314f_row8_col8,#T_1314f_row9_col8,#T_1314f_row11_col8,#T_1314f_row12_col8,#T_1314f_row13_col8{ text-align: left; text-align: left; background-color: lightgrey; }#T_1314f_row10_col8{ text-align: left; text-align: left; background-color: yellow; background-color: lightgrey; } Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) lr Logistic Regression 0.7993 0.8409 0.5549 0.6650 0.6040 0.4713 0.4753 0.9210 lda Linear Discriminant Analysis 0.7988 0.8359 0.5661 0.6592 0.6082 0.4741 0.4771 0.0360 ridge Ridge Classifier 0.7955 0.0000 0.5163 0.6692 0.5821 0.4499 0.4570 0.0200 gbc Gradient Boosting Classifier 0.7905 0.8356 0.5198 0.6526 0.5780 0.4412 0.4466 2.2610 lightgbm Light Gradient Boosting Machine 0.7841 0.8213 0.5146 0.6363 0.5686 0.4268 0.4314 17.6720 ada Ada Boost Classifier 0.7815 0.8131 0.5180 0.6276 0.5670 0.4228 0.4266 0.6080 rf Random Forest Classifier 0.7796 0.8123 0.4623 0.6403 0.5364 0.3970 0.4063 0.6200 svm SVM - Linear Kernel 0.7782 0.0000 0.5679 0.6076 0.5812 0.4322 0.4361 0.0340 xgboost Extreme Gradient Boosting 0.7770 0.8045 0.4914 0.6231 0.5486 0.4035 0.4090 8.3540 et Extra Trees Classifier 0.7729 0.7858 0.4477 0.6250 0.5210 0.3778 0.3871 0.3010 nb Naive Bayes 0.7661 0.7751 0.6235 0.5720 0.5961 0.4320 0.4332 0.0110 knn K Neighbors Classifier 0.7625 0.7733 0.5198 0.5789 0.5472 0.3870 0.3884 0.1030 qda Quadratic Discriminant Analysis 0.7492 0.8147 0.7213 0.5354 0.6142 0.4346 0.4455 0.0180 dt Decision Tree Classifier 0.7345 0.6807 0.5326 0.5200 0.5259 0.3417 0.3419 0.1380","title":"Compare Models"},{"location":"showcase/customer-churn/04_modeling/04_modeling/#print-best-model","text":"print(best) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2', random_state=42, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)","title":"Print 'Best' model"},{"location":"showcase/customer-churn/04_modeling/04_modeling/#create-model","text":"#T_2c6c0_row10_col0,#T_2c6c0_row10_col1,#T_2c6c0_row10_col2,#T_2c6c0_row10_col3,#T_2c6c0_row10_col4,#T_2c6c0_row10_col5,#T_2c6c0_row10_col6{ background: yellow; } Accuracy AUC Recall Prec. F1 Kappa MCC 0 0.7607 0.8181 0.4359 0.5930 0.5025 0.3497 0.3569 1 0.8057 0.8357 0.5470 0.6882 0.6095 0.4824 0.4881 2 0.8128 0.8565 0.6239 0.6759 0.6489 0.5215 0.5223 3 0.8175 0.8723 0.5556 0.7222 0.6280 0.5099 0.5176 4 0.8009 0.8393 0.5897 0.6571 0.6216 0.4871 0.4884 5 0.7981 0.8504 0.6121 0.6396 0.6256 0.4874 0.4877 6 0.8029 0.8241 0.5690 0.6667 0.6140 0.4827 0.4854 7 0.7957 0.8301 0.5345 0.6596 0.5905 0.4564 0.4609 8 0.8266 0.8501 0.5690 0.7416 0.6439 0.5319 0.5401 9 0.7720 0.8328 0.5128 0.6061 0.5556 0.4036 0.4062 Mean 0.7993 0.8409 0.5549 0.6650 0.6040 0.4713 0.4753 SD 0.0189 0.0156 0.0510 0.0438 0.0423 0.0529 0.0527 lr_final = finalize_model(lr_model) save_model(lr_final,'lr_20210701') Finished loading model, total used 100 iterations Transformation Pipeline and Model Succesfully Saved","title":"Create Model"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/","text":"Evaluation Stage 5 Evaluation learning models use a feedback process to learn about how accurate it is. They do this by comparing their predicted value with the actual value and then adjusting their prediction based on what they have learned so far. This feedback process allows evaluation learning models to be more accurate than other machine learning algorithms because they can be updated with new information. Learning Features Confusion Matrix Class Report Errors AUC/ROC Validation Curve Precision/Recall Calibration Boundary Threshold Dimension Recursive Feature Selection","title":"Evaluation"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#evaluation","text":"Stage 5 Evaluation learning models use a feedback process to learn about how accurate it is. They do this by comparing their predicted value with the actual value and then adjusting their prediction based on what they have learned so far. This feedback process allows evaluation learning models to be more accurate than other machine learning algorithms because they can be updated with new information.","title":"Evaluation"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#learning","text":"","title":"Learning"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#features","text":"","title":"Features"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#class-report","text":"","title":"Class Report"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#errors","text":"","title":"Errors"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#aucroc","text":"","title":"AUC/ROC"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#validation-curve","text":"","title":"Validation Curve"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#precisionrecall","text":"","title":"Precision/Recall"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#calibration","text":"","title":"Calibration"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#boundary","text":"","title":"Boundary"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#threshold","text":"","title":"Threshold"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#dimension","text":"","title":"Dimension"},{"location":"showcase/customer-churn/05_evaluation/05_evaluation/#recursive-feature-selection","text":"","title":"Recursive Feature Selection"},{"location":"showcase/customer-churn/06_deployment/06_deployment/","text":"Deployment Stage 6 A model is not particularly useful unless the customer can access its results. The complexity of this phase varies widely. This final phase has four tasks: Plan deployment: Develop and document a plan for deploying the model. Plan monitoring and maintenance: Develop a thorough monitoring and maintenance plan to avoid issues during the operational phase (or post-project phase) of a model. Produce final report: The project team documents a summary of the project which might include a final presentation of data mining results. Review project: Conduct a project retrospective about what went well, what could have been better, and how to improve in the future. The deployment of machine learning models is the process of making models available in production where web applications, enterprise software, and APIs can consume the trained model by providing new data points and generating predictions. Normally machine learning models are built so that they can be used to predict an outcome (binary value i.e. 1 or 0 for Classification, continuous values for Regression, labels for Clustering, etc. There are two broad ways of generating predictions (i) predict by batch; and (ii) predict in real-time. This tutorial will show how you can deploy your machine learning models as API to predict in real-time. It doesn't actually stop there, if the model is going to production, be sure you maintain the model in production. Constant monitoring and occasional model tuning is often required.","title":"Deployment"},{"location":"showcase/customer-churn/06_deployment/06_deployment/#deployment","text":"Stage 6 A model is not particularly useful unless the customer can access its results. The complexity of this phase varies widely. This final phase has four tasks: Plan deployment: Develop and document a plan for deploying the model. Plan monitoring and maintenance: Develop a thorough monitoring and maintenance plan to avoid issues during the operational phase (or post-project phase) of a model. Produce final report: The project team documents a summary of the project which might include a final presentation of data mining results. Review project: Conduct a project retrospective about what went well, what could have been better, and how to improve in the future. The deployment of machine learning models is the process of making models available in production where web applications, enterprise software, and APIs can consume the trained model by providing new data points and generating predictions. Normally machine learning models are built so that they can be used to predict an outcome (binary value i.e. 1 or 0 for Classification, continuous values for Regression, labels for Clustering, etc. There are two broad ways of generating predictions (i) predict by batch; and (ii) predict in real-time. This tutorial will show how you can deploy your machine learning models as API to predict in real-time. It doesn't actually stop there, if the model is going to production, be sure you maintain the model in production. Constant monitoring and occasional model tuning is often required.","title":"Deployment"},{"location":"showcase/discussion-board/discussion-board/","text":"Discussion Board Analysis Learning Analytics The world is changing. Communication is moving to digital boards, and communities are coalescing around these boards. They have also become a common interactive tool in the learning and teaching environment. Discussion Boards are an interesting place to examine how people interact with each other online using different strategies that may be best suited to their personality or position on a topic. With this notebook, you can easily visualize and understand the interaction and mood of the discussion. Work Flow (External) Export Blackboard course to local folder, ensure uncompressed. Extract posts form the blackboard course Clean posts Plot interaction between students (eyeball plot) (Not shown) Calculate Word frequencies and Word Clouds (Not shown) Summarize Author Posting frequency Calculate Polarity, Subjective, Negative, Neutral, Positive, and Compound Create Polarity and Subjectivity Box Plots Use sentinment dimension to plot the 'Shape' .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Subjectivity Polarity Negative Neutral Positive Compound author 0 0.182000 0.068000 0.029000 0.901500 0.069500 0.220200 1 0.279583 0.061736 0.000000 0.920000 0.080000 0.485100 2 0.490774 0.261210 0.000000 0.747667 0.252333 0.678867 3 0.795795 0.045808 0.035000 0.656000 0.308500 0.894850 4 0.646389 0.252500 0.024667 0.669667 0.306000 0.645167 ... ... ... ... ... ... ... 65 0.431597 0.059028 0.000000 0.837500 0.162500 0.614950 66 0.490476 0.213095 0.021000 0.696500 0.283000 0.711900 67 0.450000 0.281250 0.000000 0.888000 0.112000 0.490300 68 0.213636 0.059091 0.000000 0.881500 0.118500 0.473850 69 0.231888 0.156378 0.039500 0.756000 0.204000 0.594050 70 rows \u00d7 6 columns The polarity is a float between -1 and 1, where -1 is a negative statement and 1 is a positive statement. From the above, we can see the IMDB statement is deemed as negative, but not heavily so, and the Twitter statement is very positive. The subjectivity is TextBlobs score of whether the statement is deemed as more opinion, or fact based. A higher subjectivity score means it is less objective, and therefore would be highly opinionated.","title":"Blackboard Discussion"},{"location":"showcase/discussion-board/discussion-board/#discussion-board-analysis","text":"Learning Analytics The world is changing. Communication is moving to digital boards, and communities are coalescing around these boards. They have also become a common interactive tool in the learning and teaching environment. Discussion Boards are an interesting place to examine how people interact with each other online using different strategies that may be best suited to their personality or position on a topic. With this notebook, you can easily visualize and understand the interaction and mood of the discussion.","title":"Discussion Board Analysis"},{"location":"showcase/discussion-board/discussion-board/#work-flow","text":"(External) Export Blackboard course to local folder, ensure uncompressed. Extract posts form the blackboard course Clean posts Plot interaction between students (eyeball plot) (Not shown) Calculate Word frequencies and Word Clouds (Not shown) Summarize Author Posting frequency Calculate Polarity, Subjective, Negative, Neutral, Positive, and Compound Create Polarity and Subjectivity Box Plots Use sentinment dimension to plot the 'Shape' .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Subjectivity Polarity Negative Neutral Positive Compound author 0 0.182000 0.068000 0.029000 0.901500 0.069500 0.220200 1 0.279583 0.061736 0.000000 0.920000 0.080000 0.485100 2 0.490774 0.261210 0.000000 0.747667 0.252333 0.678867 3 0.795795 0.045808 0.035000 0.656000 0.308500 0.894850 4 0.646389 0.252500 0.024667 0.669667 0.306000 0.645167 ... ... ... ... ... ... ... 65 0.431597 0.059028 0.000000 0.837500 0.162500 0.614950 66 0.490476 0.213095 0.021000 0.696500 0.283000 0.711900 67 0.450000 0.281250 0.000000 0.888000 0.112000 0.490300 68 0.213636 0.059091 0.000000 0.881500 0.118500 0.473850 69 0.231888 0.156378 0.039500 0.756000 0.204000 0.594050 70 rows \u00d7 6 columns The polarity is a float between -1 and 1, where -1 is a negative statement and 1 is a positive statement. From the above, we can see the IMDB statement is deemed as negative, but not heavily so, and the Twitter statement is very positive. The subjectivity is TextBlobs score of whether the statement is deemed as more opinion, or fact based. A higher subjectivity score means it is less objective, and therefore would be highly opinionated.","title":"Work Flow"},{"location":"showcase/employee-uplift/employee-uplift/","text":"Employee Uplift Predictive Analytics Employee retention is a challenge for many companies. This is due to the intense market competition and the high demand for talent. A recent study shows that employees who feel like they are part of a community at work are more engaged, more productive, and less likely to leave. Conventional predictive model have an average prediction accuracy of 84%; it only yields a success rate of 50% when those identified received a retention program. Some stay and didn't receive intervention, other left even though they received intervention. Retention program cost money, so how can the business fine tune the process to optmise the results of the retention program. Wijaya et al.[1] propose a uplift model which only yields an average accuracy of 67% but yields a consistent success rate of 100% in targeting the right employee with a retention program. Acknowledgement [1] D. Wijaya, J. H. DS, S. Barus, B. Pasaribu, L. I. Sirbu, and A. Dharma, \u201cUplift modeling VS conventional predictive model: A reliable machine learning model to solve employee turnover,\u201d Int. J. Artif. Intell. Res. Vol 5, No 1 Artic. Press, 2021, doi: 10.29099/ijair.v4i2.169.","title":"Employee Uplift"},{"location":"showcase/employee-uplift/employee-uplift/#employee-uplift","text":"Predictive Analytics Employee retention is a challenge for many companies. This is due to the intense market competition and the high demand for talent. A recent study shows that employees who feel like they are part of a community at work are more engaged, more productive, and less likely to leave. Conventional predictive model have an average prediction accuracy of 84%; it only yields a success rate of 50% when those identified received a retention program. Some stay and didn't receive intervention, other left even though they received intervention. Retention program cost money, so how can the business fine tune the process to optmise the results of the retention program. Wijaya et al.[1] propose a uplift model which only yields an average accuracy of 67% but yields a consistent success rate of 100% in targeting the right employee with a retention program.","title":"Employee Uplift"},{"location":"showcase/employee-uplift/employee-uplift/#acknowledgement","text":"[1] D. Wijaya, J. H. DS, S. Barus, B. Pasaribu, L. I. Sirbu, and A. Dharma, \u201cUplift modeling VS conventional predictive model: A reliable machine learning model to solve employee turnover,\u201d Int. J. Artif. Intell. Res. Vol 5, No 1 Artic. Press, 2021, doi: 10.29099/ijair.v4i2.169.","title":"Acknowledgement"},{"location":"showcase/pca-analysis/pca-analysis/","text":"Automated PCA Analysis PCA is a technique for transforming a set of variables into a set of uncorrelated variables. PCA can be used for exploratory data analysis, dimensionality reduction, feature extraction, and many other tasks. PCA is a linear transformation so it can be implemented efficiently and it\u2019s the most commonly used algorithm for dimensionality reduction. This web app tool allows you to upload a CSV and then plot two components, automating the building of PCA plots. Waiting for CSV file Automated Plot","title":"Automated PCA"},{"location":"showcase/pca-analysis/pca-analysis/#automated-pca-analysis","text":"PCA is a technique for transforming a set of variables into a set of uncorrelated variables. PCA can be used for exploratory data analysis, dimensionality reduction, feature extraction, and many other tasks. PCA is a linear transformation so it can be implemented efficiently and it\u2019s the most commonly used algorithm for dimensionality reduction. This web app tool allows you to upload a CSV and then plot two components, automating the building of PCA plots.","title":"Automated PCA Analysis"},{"location":"showcase/pca-analysis/pca-analysis/#waiting-for-csv-file","text":"","title":"Waiting for CSV file"},{"location":"showcase/pca-analysis/pca-analysis/#automated-plot","text":"","title":"Automated Plot"},{"location":"showcase/simple-sentiment/simple-sentiment/","text":"Sentiment Analysis using Textblob (NLP for beginners) Text Analytics Text analytics provides a powerful way to derive meaning from textual data. It can be used for a number of different things such as sentiment analysis, topic detection, and predictive analysis. The most common way that text analytics is used is to extract information about the topic and sentiment of any given piece of text. This allows companies to examine how their customers feel about their company or products. Text analytics has been around for a number of years now, however it has exploded in popularity thanks to recent advancements in machine learning algorithms and the availability of affordable cloud computing. Workflow Load & Clean data Install TextBlob (Optional) Perform sentiment analysis 1. Load & Clean data import pandas as pd import re # Could use text cleaner (lighweigtht) or nltk and remove stopwords etc.. def clean_text(text): # Comment out any line for textual elements you want to keep text = re.sub('<[^<]+?>', '', text) # strip html text = re.sub('\\\\xa0','', text) # remove some rogue code from posts text = re.sub(r'[^\\w\\s]','',text) # remove puncuation text = re.sub(r'\\d+', '', text) # remove numbers text = \" \".join(text.split()) # remove extra whitespace text = text.lower() # to lowercase return text # Load the data df = pd.read_csv('posts.csv') print(\"Before Cleaning\") display(df.head()) # Clean the data df.post = df.post.apply(clean_text) print(\"After Cleaning\") df.head() Before Cleaning .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } post 0 <p>Are there any key differences in the procur... 1 <p>The concept of procurement is considerably ... 2 <p>What are the roles and responsibilities of ... 3 <p>The Chief Procurement Officer is head of th... 4 <p>How does governance play part in the ICT pr... After Cleaning .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } post 0 are there any key differences in the procureme... 1 the concept of procurement is considerably sim... 2 what are the roles and responsibilities of a c... 3 the chief procurement officer is head of the p... 4 how does governance play part in the ict procu... 2. Install TextBlob (Optional) If not installed, uncomment preferred method below. See https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/ for why using sys method # Install a conda package in the current Jupyter kernel #import sys #!conda install --yes --prefix {sys.prefix} textblob # Install a pip package in the current Jupyter kernel #import sys #!{sys.executable} -m pip install textblob 3. Perform Sentiment Analysis The sentiment function of textblob returns two properties, polarity, and subjectivity. Polarity Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjectivity Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1]. from textblob import TextBlob # Wrapper so can use Pandas apply() function on a column def getSubjectivity(text): return TextBlob(text).sentiment.subjectivity def getPolarity(text): return TextBlob(text).sentiment.polarity # Could use lambda function and not need wrappers above, but I think the wrapper # method is more readable and matches the applicaiton of the clean_text() function above # df['subjectivity'] = df.post.apply(lambda x: TextBlob(x).sentiment.subjectivity) # Calculate sentiment, df['subjectivity'] = df.post.apply(getSubjectivity) df['polarity'] = df.post.apply(getPolarity) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } post subjectivity polarity 0 are there any key differences in the procureme... 0.800000 0.000000 1 the concept of procurement is considerably sim... 0.358333 0.163542 2 what are the roles and responsibilities of a c... 0.000000 0.000000 3 the chief procurement officer is head of the p... 0.587963 0.377778 4 how does governance play part in the ict procu... 0.000000 0.000000 df.boxplot(column=['subjectivity','polarity'], grid=False, figsize=(12,8)) <AxesSubplot:>","title":"Sentiment Analysis using Textblob (NLP for beginners)"},{"location":"showcase/simple-sentiment/simple-sentiment/#sentiment-analysis-using-textblob-nlp-for-beginners","text":"Text Analytics Text analytics provides a powerful way to derive meaning from textual data. It can be used for a number of different things such as sentiment analysis, topic detection, and predictive analysis. The most common way that text analytics is used is to extract information about the topic and sentiment of any given piece of text. This allows companies to examine how their customers feel about their company or products. Text analytics has been around for a number of years now, however it has exploded in popularity thanks to recent advancements in machine learning algorithms and the availability of affordable cloud computing.","title":"Sentiment Analysis using Textblob (NLP for beginners)"},{"location":"showcase/simple-sentiment/simple-sentiment/#workflow","text":"Load & Clean data Install TextBlob (Optional) Perform sentiment analysis","title":"Workflow"},{"location":"showcase/simple-sentiment/simple-sentiment/#1-load-clean-data","text":"import pandas as pd import re # Could use text cleaner (lighweigtht) or nltk and remove stopwords etc.. def clean_text(text): # Comment out any line for textual elements you want to keep text = re.sub('<[^<]+?>', '', text) # strip html text = re.sub('\\\\xa0','', text) # remove some rogue code from posts text = re.sub(r'[^\\w\\s]','',text) # remove puncuation text = re.sub(r'\\d+', '', text) # remove numbers text = \" \".join(text.split()) # remove extra whitespace text = text.lower() # to lowercase return text # Load the data df = pd.read_csv('posts.csv') print(\"Before Cleaning\") display(df.head()) # Clean the data df.post = df.post.apply(clean_text) print(\"After Cleaning\") df.head() Before Cleaning .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } post 0 <p>Are there any key differences in the procur... 1 <p>The concept of procurement is considerably ... 2 <p>What are the roles and responsibilities of ... 3 <p>The Chief Procurement Officer is head of th... 4 <p>How does governance play part in the ICT pr... After Cleaning .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } post 0 are there any key differences in the procureme... 1 the concept of procurement is considerably sim... 2 what are the roles and responsibilities of a c... 3 the chief procurement officer is head of the p... 4 how does governance play part in the ict procu...","title":"1. Load &amp; Clean data"},{"location":"showcase/simple-sentiment/simple-sentiment/#2-install-textblob-optional","text":"If not installed, uncomment preferred method below. See https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/ for why using sys method # Install a conda package in the current Jupyter kernel #import sys #!conda install --yes --prefix {sys.prefix} textblob # Install a pip package in the current Jupyter kernel #import sys #!{sys.executable} -m pip install textblob","title":"2. Install TextBlob (Optional)"},{"location":"showcase/simple-sentiment/simple-sentiment/#3-perform-sentiment-analysis","text":"The sentiment function of textblob returns two properties, polarity, and subjectivity.","title":"3. Perform Sentiment Analysis"},{"location":"showcase/simple-sentiment/simple-sentiment/#polarity","text":"Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement.","title":"Polarity"},{"location":"showcase/simple-sentiment/simple-sentiment/#subjectivity","text":"Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1]. from textblob import TextBlob # Wrapper so can use Pandas apply() function on a column def getSubjectivity(text): return TextBlob(text).sentiment.subjectivity def getPolarity(text): return TextBlob(text).sentiment.polarity # Could use lambda function and not need wrappers above, but I think the wrapper # method is more readable and matches the applicaiton of the clean_text() function above # df['subjectivity'] = df.post.apply(lambda x: TextBlob(x).sentiment.subjectivity) # Calculate sentiment, df['subjectivity'] = df.post.apply(getSubjectivity) df['polarity'] = df.post.apply(getPolarity) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } post subjectivity polarity 0 are there any key differences in the procureme... 0.800000 0.000000 1 the concept of procurement is considerably sim... 0.358333 0.163542 2 what are the roles and responsibilities of a c... 0.000000 0.000000 3 the chief procurement officer is head of the p... 0.587963 0.377778 4 how does governance play part in the ict procu... 0.000000 0.000000 df.boxplot(column=['subjectivity','polarity'], grid=False, figsize=(12,8)) <AxesSubplot:>","title":"Subjectivity"},{"location":"showcase/visual-decisions/visual-decisions/","text":"Visual Decision Making - People who make decisions need to know what information is relevant and what is not. Plotting your data can help you spot patterns and trends that you might not otherwise notice, and help you make better decisions. Clustering is an unsupervised learning strategy. Because there is no ground truth, checking the quality of clustering can be challenging. These plots are not for detailed analysis, they are used to visually select models and can be used a point of discussion. So no axis tick marks, legends or other distracting elements. Projection into PCA space The survey data has one-hot-encoded and projected into PCA space. The transformation into PCA is good when the first two components are plotted against each other randomly fill the plot. Let us plot the two most significant principal components (PC) and colour each by the cluster. If the PCs are right, we should see a reasonable separation of the clusters. 'Black' means noise. Based on the initial scatter plots, let us plot the potential models. This plot is called the 'Silhouette' plot and is a visual way to interpret the validation and consistency within clusters of data. Each group has its colour. Generally, we want the clusters to be balanced, and roughly the same size, but small clusters can sometimes identify outlier or interesting cluster that needs further investigation. Any 'tails' to the left suggest a poor match of some data to the cluster (outliers?) so we want these to be small I call the following plot the 'barcode' plot (my name, not official). I take the labels (cluster ids) and map them back to the original survey data. Then for each cluster, I calculate the 'mode' of each question. The mode represents the average answers of each question for each cluster. Since each question has an integer encoding, I assign each a colour. So visually, we can compare the average answers of each group. So in this plot, you are looking for 'difference' to the band above Projection into AE space Survey data has been projected into Autoencode (AE) space. Let us plot the first two most compoments and colour each by the cluster. If the AE is good, we should see reasonable seperation of the clusters. 'Black' mean noise Based on the initial scatter plots, let us plot the potential models. This plot is called the 'Silhouette' plot and is a visual way to interpret the validation and consistency within clusters of data. Each group has its colour. Generally, we want the clusters to be balanced, and roughly the same size, but small clusters can sometimes identify outlier or interesting cluster that needs further investigation. Any 'tails' to the left suggest a poor match of some data to the cluster (outliers?) so we want these to be small I call the following plot the 'barcode' plot (my name, not official). I take the labels (cluster ids) and map them back to the original survey data. Then for each cluster, I calculate the 'mode' of each question. The mode represents the average answers of each question for each cluster. Since each question has an integer encoding, I assign each a colour. So visually, we can compare the average answers of each group. So in this plot, you are looking for 'difference' to the band above","title":"Visual Decisions"},{"location":"showcase/visual-decisions/visual-decisions/#visual-decision-making","text":"- People who make decisions need to know what information is relevant and what is not. Plotting your data can help you spot patterns and trends that you might not otherwise notice, and help you make better decisions. Clustering is an unsupervised learning strategy. Because there is no ground truth, checking the quality of clustering can be challenging. These plots are not for detailed analysis, they are used to visually select models and can be used a point of discussion. So no axis tick marks, legends or other distracting elements.","title":"Visual Decision Making"},{"location":"showcase/visual-decisions/visual-decisions/#projection-into-pca-space","text":"The survey data has one-hot-encoded and projected into PCA space. The transformation into PCA is good when the first two components are plotted against each other randomly fill the plot. Let us plot the two most significant principal components (PC) and colour each by the cluster. If the PCs are right, we should see a reasonable separation of the clusters. 'Black' means noise. Based on the initial scatter plots, let us plot the potential models. This plot is called the 'Silhouette' plot and is a visual way to interpret the validation and consistency within clusters of data. Each group has its colour. Generally, we want the clusters to be balanced, and roughly the same size, but small clusters can sometimes identify outlier or interesting cluster that needs further investigation. Any 'tails' to the left suggest a poor match of some data to the cluster (outliers?) so we want these to be small I call the following plot the 'barcode' plot (my name, not official). I take the labels (cluster ids) and map them back to the original survey data. Then for each cluster, I calculate the 'mode' of each question. The mode represents the average answers of each question for each cluster. Since each question has an integer encoding, I assign each a colour. So visually, we can compare the average answers of each group. So in this plot, you are looking for 'difference' to the band above","title":"Projection into PCA space"},{"location":"showcase/visual-decisions/visual-decisions/#projection-into-ae-space","text":"Survey data has been projected into Autoencode (AE) space. Let us plot the first two most compoments and colour each by the cluster. If the AE is good, we should see reasonable seperation of the clusters. 'Black' mean noise Based on the initial scatter plots, let us plot the potential models. This plot is called the 'Silhouette' plot and is a visual way to interpret the validation and consistency within clusters of data. Each group has its colour. Generally, we want the clusters to be balanced, and roughly the same size, but small clusters can sometimes identify outlier or interesting cluster that needs further investigation. Any 'tails' to the left suggest a poor match of some data to the cluster (outliers?) so we want these to be small I call the following plot the 'barcode' plot (my name, not official). I take the labels (cluster ids) and map them back to the original survey data. Then for each cluster, I calculate the 'mode' of each question. The mode represents the average answers of each question for each cluster. Since each question has an integer encoding, I assign each a colour. So visually, we can compare the average answers of each group. So in this plot, you are looking for 'difference' to the band above","title":"Projection into AE space"}]}